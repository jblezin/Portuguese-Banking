# -*- coding: utf-8 -*-
"""PortugueseBanking.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1tRGZSAYU1Zi7KyKbUpyiAhoiRHc6Lxw6
"""

#Loading essential libraries for data cleaning, exploratory analysis, and ML modeling.
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import r2_score

"""# Load Raw Data"""

#The objective of this project is identifying clients that will sign-up for a term account.
#Load raw data & view few rows in the dataset.
#Precision is the metric that will drive the model performance 

PBank = pd.read_csv('/content/sample_data/PortugueseBanking.csv')
PBank.head(6)

"""# Cleaning The Data


> Obtain a better understanding of dataset.
"""

#Checking data types
#Total of 45,211 records
PBank.info()

#This heatmap identifies null values in dataset.
#The result shows there are no null values
plt.figure(figsize = (10,8))
sns.heatmap(PBank.isnull(), cbar = False, cmap = 'viridis')

"""# Convert and transform data for easier analysis


> Create a unified dataset.
"""

#Quick glance at potential predictive features
pd.crosstab(PBank['job'],PBank['y']).plot(kind = 'bar',color=['c', 'r'])
pd.crosstab(PBank['contact'],PBank['y']).plot(kind = 'bar',color=['c', 'r'])
pd.crosstab(PBank['marital'],PBank['y']).plot(kind = 'bar',color=['c', 'r'])
pd.crosstab(PBank['month'],PBank['y']).plot(kind = 'bar',color=['c', 'r'])

#Creating dummy variables for categorical data. 
PBank = pd.get_dummies(PBank, columns = ['marital', 'education', 'poutcome','job'])

#Check results
PBank.head(4)

#Convert columns with 'yes' or 'no' responses to corresponding numerical value.
Default = {'yes': 0, 'no': 1}
PBank['default'] = PBank['default'].map(Default)

House = {'yes': 0, 'no': 1}
PBank['housing'] = PBank['housing'].map(House)

Loan = {'yes': 0, 'no': 1}
PBank['loan'] = PBank['loan'].map(Loan)

Y = {'yes': 0, 'no': 1}
PBank['y'] = PBank['y'].map(Y)

#Check results
PBank.head(4)

#Change the unit of 'duration' from seconds to minutes
PBank['duration'] = PBank['duration'].apply(lambda n:n/60).round(2)

#Check results
PBank.head(3)

#Count of target values. 0 = yes 1 = no
PBank['y'].value_counts().plot(kind ='bar', color = ['c', 'r'])

"""# Exploratory Analysis


> Visualizing the distribution and relationship between key features.
"""

#Remove clients with a negative account balance
Negative_Balance = (PBank['balance'] < 0)
PBank = PBank.drop(PBank[Negative_Balance].index, axis = 0, inplace = False)
PBank.head(2)

#Distribution of age and balance
plt.figure(figsize = (8,6))
PBank['age'].plot(kind = 'hist')
plt.title('The Distribution of Age')


#Distribution of age and balance
plt.figure(figsize = (8,6))
PBank['balance'].plot(kind = 'hist', color = 'skyblue')
plt.title('The Distribution of Balance')

"""# Logistical Regression Approach

> Train Logistical Regression model and analyze feature importance through coefficients.
"""

from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score,confusion_matrix
from sklearn.linear_model import LogisticRegression

#Update Features
X = PBank.drop(['y', 'contact', 'month', 'default', 'balance', 'loan', 
                'job_student','job_unknown','day', 'pdays','age','housing', 'previous'], axis = 1)
y = PBank['y']

X_train, X_test, y_train, y_test = train_test_split(X, y, train_size = .2, random_state = 42)


#Class imbalance with Y target column 
from imblearn.over_sampling import SMOTE

OS = SMOTE(random_state = 42)

X_train_smote, y_train_smote = OS.fit_sample(X_train, y_train)

print("length of oversampled data is ",len(X_train_smote))
print("Number of no subscription in oversampled data",sum(y_train_smote == 0))
print("Number of subscription",sum(y_train_smote == 1))


#Create a scaler
#Logistical Regression has regularization
std_scale = StandardScaler()
X_train_scaled = std_scale.fit_transform(X_train_smote)

#Only transform on test data
X_test_scaled = std_scale.transform(X_test)


#Create, fit, and predict on model
PBank_model = LogisticRegression(solver = 'lbfgs', max_iter = 2000)
Pbank_fit = PBank_model.fit(X_train_scaled, y_train_smote)


#Prediction with X_test
y_predict = (PBank_model.predict_proba(X_test_scaled)[:, 0] < 0.1)
           

#Score
PBank_model.score(X_train_scaled, y_train_smote)

from sklearn.metrics import accuracy_score, classification_report, precision_score, recall_score, roc_curve, auc

#Precision is the metric that will drive the model performance 
#The accuracy, precision, recall and f1 scores
print('Accuracy :', accuracy_score(y_test, y_predict))
print('\n')
print(classification_report(y_test, y_predict))

#Look at the coefficient and intercept of the model:
a = (list(zip(PBank_model.coef_[0], X.columns)))

coef = pd.DataFrame(a, columns = ['coef', 'features'])
coef

#Export to incorporate in Tableau dashboard
coef.to_excel("output.xlsx")

#Variable for coefficients
importance = PBank_model.coef_[0]

#Summarize feature importance
for i,v in zip(coef['features'],importance):
	print(f'Feature: {i}, Score: {v}')

#Plot feature importance
plt.figure(figsize=(30,20))
plt.bar(coef['features'], importance)
plt.xticks(rotation=90)

#Confusion Matrix
print(confusion_matrix(y_test, y_predict))

#Print confusion matrix for Log
log_confusion = confusion_matrix(y_test, y_predict)
plt.figure(dpi=150)
sns.heatmap(log_confusion, cmap=plt.cm.Blues, annot=True, square=True)

#Labels
plt.xlabel('Predicted Term Accounts')
plt.ylabel('Actual Term Accounts')
plt.title('Log confusion matrix');